# Web-Scraping (Client Project)

# About the Project
Our goal is to scrape a watch brandâ€™s website to get a set of data points about every watch. 

![image](https://github.com/ssq-94/Web-Scraping/assets/78969075/25ec0299-f525-4c68-be38-c6f35ac4a221)


We have 3 major tasks broken into different stages:

# Stage 1
- Data collection via web scraping.
- Data analysis - create a summary report in Python notebook.

# Stage 2 
- Data cleaning

# Stage 3 
- Pipeline automation using Airflow and Docker.


# Stage 1 Project Steps

## Step 1: Installations
To start scraping, you'll need to install the items below.
- `selenium`- python library
- `webdriver`- for Google Chrome browser
  
#### More Info

- `selenium:` Used for web automation and controlling web browsers programmatically.
- `webdriver:` Provides a way to start and control a web browser using Python.
- `By:` Provides mechanisms for locating elements on a web page.
- `WebDriverWait and expected_conditions:` Used for waiting until certain conditions are met before proceeding with code execution.
- `Keys:` Provides special keys (e.g., Enter, Shift) for simulating keyboard actions.
- `BeautifulSoup:` Used for parsing HTML and XML documents and extracting data from web pages.
- `time:` Provides time-related functions, such as adding delays in code execution.
- `requests:` Used for making HTTP requests to retrieve web pages or resources.
- `csv:` Provides functionality for reading and writing CSV files.
- `pandas:` Widely used for data manipulation and analysis, with DataFrames for working with tabular data.
- `pd.set_option('display.max_columns', None):` Sets an option in pandas to display all columns when printing DataFrames.
- `re:` Provides support for regular expressions, used for pattern matching and text manipulation.
